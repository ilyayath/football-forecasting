{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXAmFAWZSe0DtWmZP/iEFT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilyayath/football-forecasting/blob/main/parsing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parsing of site [FBREF](https://https://fbref.com/en/)**"
      ],
      "metadata": {
        "id": "MrwNtS_VdE7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Szc1v746quHp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup as Bs\n",
        "import csv\n",
        "def pars(url,div_id,table_id,file_name):\n",
        "  scraper = cloudscraper.create_scraper()\n",
        "  html = scraper.get(url).text\n",
        "  soup = Bs(html, \"html.parser\")\n",
        "  divs = soup.find_all('div', id=div_id)\n",
        "\n",
        "  if divs:\n",
        "    div = divs[0]\n",
        "    table = div.find('table', id=table_id)\n",
        "\n",
        "    if table:\n",
        "      headers = [th.text for th in table.find_all('th')]\n",
        "      data = []\n",
        "      for row in table.find_all('tr')[1:]:\n",
        "        cols = []\n",
        "        for cell in row.find_all(['th', 'td']):\n",
        "            cols.append(cell.text.strip())\n",
        "        if cols:\n",
        "            data.append(cols)\n",
        "\n",
        "    filtered_headers = []\n",
        "    filtered_data = []\n",
        "    columns_to_exclude = [\"Goalkeeper\", \"Notes\"] + [str(i) for i in range(1, 21)]\n",
        "\n",
        "    for i, header in enumerate(headers):\n",
        "        if header not in columns_to_exclude:\n",
        "            filtered_headers.append(header)\n",
        "            filtered_data.append([row[i] for row in data])\n",
        "\n",
        "    transposed_data = list(zip(*filtered_data))\n",
        "\n",
        "    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(filtered_headers)\n",
        "        writer.writerows(transposed_data)\n",
        "\n",
        "    print(f\"Data is saved in {file_name}\")\n",
        "  else:\n",
        "    print(\"Table isn`t found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup as Bs\n",
        "import csv\n",
        "\n",
        "def pars2(url, div_id, table_id, file_name):\n",
        "\n",
        "    scraper = cloudscraper.create_scraper()\n",
        "    html = scraper.get(url).text\n",
        "    soup = Bs(html, \"html.parser\")\n",
        "\n",
        "    divs = soup.find_all('div', id=div_id)\n",
        "\n",
        "    if divs:\n",
        "        div = divs[0]\n",
        "\n",
        "        table = div.find('table', id=table_id)\n",
        "\n",
        "        if table:\n",
        "\n",
        "            headers = [th.text.strip() for th in table.find_all('th')]\n",
        "            headers = headers[6:38]\n",
        "            num_header_cols = len(headers)\n",
        "\n",
        "            data = []\n",
        "            for row in table.find_all('tr')[2:]:\n",
        "                cols = [cell.text.strip() for cell in row.find_all(['th', 'td'])]\n",
        "                if cols:\n",
        "                    data.append(cols)\n",
        "\n",
        "            if data and len(data[0]) == len(headers):\n",
        "                with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    writer.writerow(headers)\n",
        "                    writer.writerows(data)\n",
        "\n",
        "                print(f\"Data is saved in {file_name}\")\n"
      ],
      "metadata": {
        "id": "CBEw3WM50Ldr"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}